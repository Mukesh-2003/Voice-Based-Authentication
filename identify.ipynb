{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import librosa\n",
    "import librosa.display\n",
    "import numpy as np\n",
    "import joblib\n",
    "from pydub import AudioSegment\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "import soundfile as sf\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpeakerIdentification:\n",
    "    def __init__(self):\n",
    "        self.samplerate = 16000\n",
    "        self.model = None\n",
    "        self.scaler = StandardScaler()\n",
    "        self.label_encoder = LabelEncoder()\n",
    "\n",
    "        # Reference speakers and their folder paths\n",
    "        self.reference_speakers = {\n",
    "            \"Kaviya\": r\"D:\\VS Workspace\\Biometrics project\\dataset\\kaviya\",\n",
    "            \"Vamsi\": r\"D:\\VS Workspace\\Biometrics project\\dataset\\vamsi\",\n",
    "            \"Lavanya\": r\"D:\\VS Workspace\\Biometrics project\\dataset\\lavanya\",\n",
    "            \"Raji\": r\"D:\\VS Workspace\\Biometrics project\\dataset\\raji\",\n",
    "            \"Raman\": r\"D:\\VS Workspace\\Biometrics project\\dataset\\raman\",\n",
    "        }\n",
    "\n",
    "        # Load the pre-trained model if it exists, otherwise train a new one\n",
    "        self.load_model()\n",
    "\n",
    "    def print_feature_importances(self):\n",
    "        \"\"\"Print the feature importances from the trained Random Forest model.\"\"\"\n",
    "        if self.model is None:\n",
    "            print(\"Model is not trained yet.\")\n",
    "            return\n",
    "        \n",
    "        rf_model = self.model[0]  # Random Forest model is stored at index 0 of self.model\n",
    "        \n",
    "        # Get feature importances from the RandomForest model\n",
    "        feature_importances = rf_model.feature_importances_\n",
    "        sorted_idx = np.argsort(feature_importances)[::-1]  # Sort by importance\n",
    "        \n",
    "        print(\"Feature importances:\")\n",
    "        for idx in sorted_idx:\n",
    "            print(f\"Feature {idx}: {feature_importances[idx]}\")\n",
    "\n",
    "    def get_audio_files(self, folder_path):\n",
    "        \"\"\"Retrieve all MP3 and WAV files from the specified folder.\"\"\"\n",
    "        return [os.path.join(folder_path, file) for file in os.listdir(folder_path) if file.endswith(\".mp3\") or file.endswith(\".wav\")]\n",
    "\n",
    "    def convert_mp3_to_wav(self, mp3_path):\n",
    "        \"\"\"Convert MP3 file to WAV format.\"\"\"\n",
    "        wav_path = mp3_path.replace(\".mp3\", \".wav\")\n",
    "        if not os.path.exists(wav_path):\n",
    "            try:\n",
    "                sound = AudioSegment.from_mp3(mp3_path)\n",
    "                sound = sound.set_channels(1)  # Convert to mono\n",
    "                sound = sound.set_frame_rate(self.samplerate)  # Convert to 16kHz\n",
    "                sound.export(wav_path, format=\"wav\")\n",
    "                print(f\"Converted {mp3_path} to {wav_path}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error converting {mp3_path} to WAV: {e}\")\n",
    "        return wav_path\n",
    "\n",
    "    def extract_features(self, file_path):\n",
    "        \"\"\"Extract audio features for speaker identification.\"\"\"\n",
    "        try:\n",
    "            audio_data, sr = librosa.load(file_path, sr=self.samplerate)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {file_path}: {e}\")\n",
    "            return None\n",
    "\n",
    "        # Extract MFCCs (Mel Frequency Cepstral Coefficients)\n",
    "        mfcc = librosa.feature.mfcc(y=audio_data, sr=sr, n_mfcc=13)\n",
    "        mfcc_delta = librosa.feature.delta(mfcc)  # Delta of MFCC\n",
    "        mfcc_delta2 = librosa.feature.delta(mfcc, order=2)  # Delta-Delta of MFCC\n",
    "\n",
    "        # Extract Chroma Features\n",
    "        chroma = librosa.feature.chroma_stft(y=audio_data, sr=sr)\n",
    "\n",
    "        # Extract Spectral Contrast\n",
    "        spectral_contrast = librosa.feature.spectral_contrast(y=audio_data, sr=sr)\n",
    "\n",
    "        # Extract Tonnetz (Harmonic Features)\n",
    "        tonnetz = librosa.feature.tonnetz(y=librosa.effects.harmonic(audio_data), sr=sr)\n",
    "\n",
    "        # Extract Zero Crossing Rate\n",
    "        zcr = librosa.feature.zero_crossing_rate(y=audio_data)\n",
    "\n",
    "        # Extract Root Mean Square Energy\n",
    "        rms = librosa.feature.rms(y=audio_data)\n",
    "\n",
    "        # Extract Mel Spectrogram\n",
    "        mel_spectrogram = librosa.feature.melspectrogram(y=audio_data, sr=sr)\n",
    "\n",
    "        # Extract Pitch (Fundamental Frequency)\n",
    "        pitches, magnitudes = librosa.core.piptrack(y=audio_data, sr=sr)\n",
    "        pitch = np.mean(pitches, axis=1)\n",
    "\n",
    "        # Combine all features into a single array (mean of each feature across time frames)\n",
    "        features = []\n",
    "\n",
    "        # Ensure each feature array has at least one value; else, use zero arrays\n",
    "        features.append(np.mean(mfcc, axis=1) if mfcc.size > 0 else np.zeros(13))  # MFCC\n",
    "        features.append(np.mean(mfcc_delta, axis=1) if mfcc_delta.size > 0 else np.zeros(13))  # Delta MFCC\n",
    "        features.append(np.mean(mfcc_delta2, axis=1) if mfcc_delta2.size > 0 else np.zeros(13))  # Delta-Delta MFCC\n",
    "        features.append(np.mean(chroma, axis=1) if chroma.size > 0 else np.zeros(12))  # Chroma\n",
    "        features.append(np.mean(spectral_contrast, axis=1) if spectral_contrast.size > 0 else np.zeros(7))  # Spectral Contrast\n",
    "        features.append(np.mean(tonnetz, axis=1) if tonnetz.size > 0 else np.zeros(6))  # Tonnetz\n",
    "        features.append(np.mean(zcr, axis=1) if zcr.size > 0 else np.zeros(1))  # Zero Crossing Rate\n",
    "        features.append(np.mean(rms, axis=1) if rms.size > 0 else np.zeros(1))  # Root Mean Square Energy\n",
    "        features.append(np.mean(mel_spectrogram, axis=1) if mel_spectrogram.size > 0 else np.zeros(mel_spectrogram.shape[0]))  # Mel Spectrogram\n",
    "        features.append(np.mean(pitch, axis=0) if pitch.size > 0 else np.zeros(1))  # Pitch\n",
    "\n",
    "        # Ensure that every feature in the list is an array with the same dimensions\n",
    "        features = [f if f.ndim > 0 else np.zeros(1) for f in features]\n",
    "\n",
    "        # Concatenate all features into a single vector\n",
    "        return np.concatenate(features)\n",
    "\n",
    "    def load_data(self):\n",
    "        \"\"\"Load audio data and extract features from the reference speakers.\"\"\"\n",
    "        X = []\n",
    "        y = []\n",
    "        for speaker, folder_path in self.reference_speakers.items():\n",
    "            audio_files = self.get_audio_files(folder_path)\n",
    "            for file_path in audio_files:\n",
    "                # Convert MP3 to WAV if necessary\n",
    "                if file_path.endswith(\".mp3\"):\n",
    "                    file_path = self.convert_mp3_to_wav(file_path)\n",
    "\n",
    "                features = self.extract_features(file_path)\n",
    "                if features is not None:\n",
    "                    X.append(features)\n",
    "                    y.append(speaker)\n",
    "\n",
    "        X = np.array(X)\n",
    "        y = np.array(y)\n",
    "        return X, y\n",
    "\n",
    "    def train_model(self):\n",
    "        \"\"\"Train the speaker identification model using RandomForest and GradientBoosting.\"\"\"\n",
    "        # Load data and encode speaker labels\n",
    "        X, y = self.load_data()\n",
    "        y_encoded = self.label_encoder.fit_transform(y)\n",
    "\n",
    "        # Split data into training and test sets\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.5, random_state=42)\n",
    "\n",
    "        # Scale the features\n",
    "        X_train_scaled = self.scaler.fit_transform(X_train)\n",
    "        X_test_scaled = self.scaler.transform(X_test)\n",
    "\n",
    "        # Apply PCA for dimensionality reduction\n",
    "        pca = PCA(n_components=0.95)  # Retain 95% of the variance\n",
    "        X_train_pca = pca.fit_transform(X_train_scaled)\n",
    "        X_test_pca = pca.transform(X_test_scaled)\n",
    "\n",
    "        # Apply LDA for supervised dimensionality reduction (with class labels)\n",
    "        lda = LDA(n_components=1)  # Reduce to 1 component for classification\n",
    "        X_train_lda = lda.fit_transform(X_train_pca, y_train)\n",
    "        X_test_lda = lda.transform(X_test_pca)\n",
    "\n",
    "        # Train RandomForest and GradientBoosting classifiers\n",
    "        rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "        gb_model = GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "        rf_model.fit(X_train_lda, y_train)\n",
    "        gb_model.fit(X_train_lda, y_train)\n",
    "\n",
    "        # Stacking: Combine predictions from both models\n",
    "        rf_preds = rf_model.predict(X_train_lda)\n",
    "        gb_preds = gb_model.predict(X_train_lda)\n",
    "\n",
    "        stacking_train_data = np.vstack([rf_preds, gb_preds]).T\n",
    "        stacking_model = LogisticRegression()\n",
    "        stacking_model.fit(stacking_train_data, y_train)\n",
    "\n",
    "        # Save the models and scaler for later use\n",
    "        self.model = (rf_model, gb_model, stacking_model, pca, lda)\n",
    "        print(f\"Model is trained successfully.\")\n",
    "\n",
    "        # Save the models and components using joblib\n",
    "        joblib.dump(self.model, 'speaker_model.pkl')\n",
    "        joblib.dump(self.scaler, 'scaler.pkl')\n",
    "        joblib.dump(self.label_encoder, 'label_encoder.pkl')\n",
    "\n",
    "        # Evaluate on the test set\n",
    "        rf_test_preds = rf_model.predict(X_test_lda)\n",
    "        gb_test_preds = gb_model.predict(X_test_lda)\n",
    "        stacking_test_data = np.vstack([rf_test_preds, gb_test_preds]).T\n",
    "        final_test_preds = stacking_model.predict(stacking_test_data)\n",
    "\n",
    "        accuracy = accuracy_score(y_test, final_test_preds)\n",
    "        print(f\"Model accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "    def load_model(self):\n",
    "        \"\"\"Load the pre-trained models and scaler.\"\"\"\n",
    "        if os.path.exists('speaker_model.pkl'):\n",
    "            self.model = joblib.load('speaker_model.pkl')\n",
    "            self.scaler = joblib.load('scaler.pkl')\n",
    "            self.label_encoder = joblib.load('label_encoder.pkl')\n",
    "            print(f\"Model loaded successfully.\")\n",
    "        else:\n",
    "            print(\"No pre-trained model found. Training a new model...\")\n",
    "            self.train_model()\n",
    "\n",
    "    def perform_identification(self, file_path):\n",
    "        \"\"\"Perform speaker identification for a given audio file.\"\"\"\n",
    "        features = self.extract_features(file_path)\n",
    "        if features is None:\n",
    "            print(\"Error extracting features from the audio file.\")\n",
    "            return\n",
    "\n",
    "        features_scaled = self.scaler.transform([features])\n",
    "\n",
    "        # Apply PCA to the input features\n",
    "        pca, lda = self.model[3], self.model[4]\n",
    "        features_pca = pca.transform(features_scaled)\n",
    "\n",
    "        # Apply LDA to the PCA-transformed features\n",
    "        features_lda = lda.transform(features_pca)\n",
    "\n",
    "        # Make predictions using both models and stack them\n",
    "        rf_model, gb_model, stacking_model = self.model[0], self.model[1], self.model[2]\n",
    "        rf_pred = rf_model.predict(features_lda)\n",
    "        gb_pred = gb_model.predict(features_lda)\n",
    "\n",
    "        stacked_data = np.vstack([rf_pred, gb_pred]).T\n",
    "        final_pred = stacking_model.predict(stacked_data)\n",
    "\n",
    "        # Convert the predicted label back to the speaker's name\n",
    "        try:\n",
    "            speaker_name = self.label_encoder.inverse_transform(final_pred)[0]\n",
    "        except IndexError:\n",
    "            speaker_name = None\n",
    "\n",
    "        # Print the result\n",
    "        if speaker_name and speaker_name in self.reference_speakers:\n",
    "            print(f\"Access granted.\")\n",
    "            print(f\"Identified Speaker: {speaker_name}\")\n",
    "        else:\n",
    "            print(\"Access denied.\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No pre-trained model found. Training a new model...\n",
      "Model is trained successfully.\n",
      "Model accuracy: 100.00%\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model\n",
    "speaker_id = SpeakerIdentification()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Access granted.\n",
      "Identified Speaker: Kaviya\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Provide the path to your pre-recorded audio file\n",
    "audio_file_path = r\"test\\kaviya6.wav\"\n",
    "\n",
    "# Perform identification\n",
    "speaker_id.perform_identification(audio_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature importances:\n",
      "Feature 0: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Assuming you have the trained model and have loaded it\n",
    "def print_feature_importances(self):\n",
    "    rf_model = self.model[0]  # Random Forest model is stored at index 0 of self.model\n",
    "\n",
    "    # Get feature importances from the RandomForest model\n",
    "    feature_importances = rf_model.feature_importances_\n",
    "    sorted_idx = np.argsort(feature_importances)[::-1]  # Sort by importance\n",
    "\n",
    "    print(\"Feature importances:\")\n",
    "    for idx in sorted_idx:\n",
    "        print(f\"Feature {idx}: {feature_importances[idx]}\")\n",
    "\n",
    "# Call the function to print feature importances\n",
    "speaker_id.print_feature_importances()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'SpeakerIdentification' object has no attribute 'pca'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m speaker_id \u001b[38;5;241m=\u001b[39m \u001b[43mSpeakerIdentification\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m speaker_id\u001b[38;5;241m.\u001b[39mtrain_model()  \u001b[38;5;66;03m# This will train and save the models\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[25], line 35\u001b[0m, in \u001b[0;36mSpeakerIdentification.__init__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mD:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mVS Workspace\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mBiometrics project\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mmodels\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# Load the pre-trained model if it exists, otherwise train a new one\u001b[39;00m\n\u001b[1;32m---> 35\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[25], line 187\u001b[0m, in \u001b[0;36mSpeakerIdentification.load_model\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    185\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabel_encoder \u001b[38;5;241m=\u001b[39m joblib\u001b[38;5;241m.\u001b[39mload(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_dir, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel_encoder.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m    186\u001b[0m \u001b[38;5;66;03m# Save PCA and LDA models\u001b[39;00m\n\u001b[1;32m--> 187\u001b[0m joblib\u001b[38;5;241m.\u001b[39mdump(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpca\u001b[49m, os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_dir, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpca_model.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m    188\u001b[0m joblib\u001b[38;5;241m.\u001b[39mdump(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlda, os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_dir, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlda_model.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m    190\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel loaded successfully.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'SpeakerIdentification' object has no attribute 'pca'"
     ]
    }
   ],
   "source": [
    "speaker_id = SpeakerIdentification()\n",
    "speaker_id.train_model()  # This will train and save the models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
